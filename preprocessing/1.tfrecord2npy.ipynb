{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Purpose:\n",
    "    1) tfrecord to be converted into npy format for pytorch running.\n",
    "    2) label index is converted into index ranging from 0 to 1000.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_validate = True # True/False for validation/test sets, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/run/media/hoosiki/WareHouse3/mtb/datasets/VU/'\n",
    "data_dir = input_dir + 'active_datasets/'\n",
    "frame_dir = data_dir + 'frame/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_validate == True:\n",
    "    datatype = 'validate'\n",
    "else:\n",
    "    datatype = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(frame_dir + '{}*.tfrecord'.format(datatype))\n",
    "out_dir   = data_dir + 'npy_formatted_frame/{}/'.format(datatype)\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab = pd.read_csv(input_dir + 'vocabulary.csv')\n",
    "vocab_label2idx_dict = {0: 0}\n",
    "for i, label in enumerate(df_vocab['Index']):\n",
    "    vocab_label2idx_dict[label] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(record):\n",
    "    context_features = {\n",
    "        'id': tf.FixedLenFeature([], tf.string),\n",
    "        'labels': tf.VarLenFeature(tf.int64),\n",
    "        'segment_start_times': tf.VarLenFeature(tf.int64),\n",
    "        'segment_end_times': tf.VarLenFeature(tf.int64),\n",
    "        'segment_labels': tf.VarLenFeature(tf.int64),\n",
    "        'segment_scores': tf.VarLenFeature(tf.float32)        \n",
    "    }\n",
    "    sequence_features = {\n",
    "        'rgb': tf.FixedLenSequenceFeature([], tf.string),\n",
    "        'audio': tf.FixedLenSequenceFeature([], tf.string)\n",
    "    }\n",
    "    contexts, sequences = tf.parse_single_sequence_example(record,\n",
    "                                                           context_features=context_features,\n",
    "                                                           sequence_features=sequence_features)\n",
    "    video_id = contexts['id']\n",
    "    video_labels = contexts['labels']\n",
    "    segment_start_times = contexts['segment_start_times']\n",
    "    segment_end_times = contexts['segment_end_times']\n",
    "    segment_labels = contexts['segment_labels']\n",
    "    segment_scores = contexts['segment_scores']\n",
    "    frame_rgb = tf.reshape(tf.cast(tf.decode_raw(sequences['rgb'], tf.uint8), tf.float32), [-1, 1024])\n",
    "    frame_audio = tf.reshape(tf.cast(tf.decode_raw(sequences['audio'], tf.uint8), tf.float32), [-1, 128])\n",
    "    return video_id, video_labels, segment_start_times, segment_end_times, segment_labels, segment_scores, frame_rgb, frame_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    for ifile in range(0, len(file_paths)):\n",
    "\n",
    "        frame_lvl_record = frame_dir + '{}%04d.tfrecord'.format(datatype) % ifile\n",
    "        print(frame_lvl_record)\n",
    "    \n",
    "        tf_dataset = tf.data.TFRecordDataset(frame_lvl_record)\n",
    "        tf_dataset = tf_dataset.map(parser)\n",
    "        iterator = tf_dataset.make_one_shot_iterator()\n",
    "        next_element = iterator.get_next()\n",
    "        try:\n",
    "            while True:\n",
    "                data_record = sess.run(next_element)\n",
    "                dataset = dict()\n",
    "                dataset['video_id'] = data_record[0].decode()\n",
    "                dataset['video_labels'] = list(data_record[1].values)\n",
    "                dataset['segment_start_times'] = list(data_record[2].values)\n",
    "                dataset['segment_end_times'] = list(data_record[3].values)\n",
    "                dataset['segment_labels'] = list(data_record[4].values)\n",
    "                dataset['segment_scores'] = list(data_record[5].values)\n",
    "                dataset['frame_rgb'] = list(data_record[6])\n",
    "                dataset['frame_audio'] = list(data_record[7])\n",
    "                for i, segment_label in enumerate(dataset['segment_labels']):\n",
    "                    dataset['segment_labels'][i] = vocab_label2idx_dict[segment_label]                    \n",
    "                np.save(out_dir + dataset['video_id'] + '.npy', np.array(dataset))\n",
    "        except:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
